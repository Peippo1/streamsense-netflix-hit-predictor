{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69f23350-00a4-4da0-a40f-8f3a6f127417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean and standardise column names\n",
    "We want consistent, lower_snake_case column names across the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a3e86f-595c-4b89-a3d3-6d446bd676f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# âš™ï¸ 02 â€“ Feature Engineering & Label Definition\n",
    "\n",
    "**Objective:**  \n",
    "Transform the ingested Netflix dataset into a clean, structured, and model-ready format by standardising column names, deriving new features, and defining the target label (`is_hit`).\n",
    "\n",
    "**Scope of this notebook:**\n",
    "- Load the raw data from the Delta table **`netflix_raw`** created in Notebook 01  \n",
    "- Standardise all column names to `snake_case` for consistency  \n",
    "- Parse and transform key fields:\n",
    "  - Extract `release_year` from `release_date`\n",
    "  - Derive numeric `duration_num` from `duration`\n",
    "  - Add a binary flag `is_movie` based on category  \n",
    "- Handle missing or null values where appropriate  \n",
    "- Define a preliminary target label `is_hit` to support downstream modelling  \n",
    "- Save the cleaned, feature-rich dataset as **`netflix_clean`** (or **`netflix_model_data`**) for use in Notebook 03\n",
    "\n",
    "**Outcome:**  \n",
    "A fully prepared dataset suitable for exploratory modelling, feature selection, and predictive analysis within Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797a3426-32be-4980-be06-84904cc4dcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start from the raw table created in Notebook 1\n",
    "df_raw = spark.table(\"netflix_raw\")\n",
    "\n",
    "def to_snake_case(name):\n",
    "    name = re.sub(r'[^0-9a-zA-Z]+', '_', name)\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "    return name.lower().strip('_')\n",
    "\n",
    "df_clean = df_raw.select(\n",
    "    [F.col(c).alias(to_snake_case(c)) for c in df_raw.columns]\n",
    ")\n",
    "\n",
    "display(df_clean.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a25510-4a2b-4642-8e12-76e3aef7e3e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define a basic â€œhitâ€ label (is_hit)\n",
    "The dataset doesnâ€™t have view counts or IMDb scores, so letâ€™s create a proxy â€œpopularityâ€ label using the text length of the description â€” purely for demonstration. Later, if we merge external IMDb data, weâ€™ll replace it with real metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8384631-c5b1-44fc-a89e-ff1081f1affc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Heuristic hit label based on description length (as a placeholder)\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"is_hit\",\n",
    "    F.when(F.length(F.col(\"description\")) > 120, 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc9e699-7322-4a84-84a0-499bd8107d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save as a Delta table\n",
    "This becomes our modelling dataset for the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5d5534-a1f4-4b99-bff7-3e406bac647f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add safe date parsing + engineered features\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    # safely parse release_date â†’ release_date_dt (NULL if parse fails)\n",
    "    .withColumn(\n",
    "        \"release_date_dt\",\n",
    "        F.expr(\"try_to_date(trim(release_date), 'MMMM d, yyyy')\")\n",
    "    )\n",
    "    # derive release_year from parsed date\n",
    "    .withColumn(\n",
    "        \"release_year\",\n",
    "        F.year(F.col(\"release_date_dt\"))\n",
    "    )\n",
    "    # numeric duration from strings like \"90 min\", \"2 Seasons\"\n",
    "    .withColumn(\n",
    "        \"duration_num\",\n",
    "        F.when(\n",
    "            F.col(\"duration\").rlike(r\"\\d+\"),                  # only if there are digits\n",
    "            F.regexp_extract(F.col(\"duration\"), r\"(\\d+)\", 1).cast(\"int\")\n",
    "        )\n",
    "        .otherwise(None)                                     # no digits -> NULL\n",
    "    )\n",
    "    # movie flag\n",
    "    .withColumn(\n",
    "        \"is_movie\",\n",
    "        F.when(F.lower(F.col(\"category\")) == \"movie\", 1).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Persist cleaned data as a Delta table\n",
    "(\n",
    "    df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"netflix_clean\")\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "display(\n",
    "    spark.table(\"netflix_clean\")\n",
    "    .select(\"title\", \"category\", \"release_year\", \"duration_num\", \"is_hit\")\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde61e4e-24a1-48f6-8b81-09e2d2b52374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ§¾ Notebook Summary â€“ 02_Feature_Engineering_And_Label\n",
    "\n",
    "**Objective:**  \n",
    "Transform the raw Netflix dataset (`netflix_raw`) into a clean, feature-rich, and model-ready format for machine learning.\n",
    "\n",
    "**Key steps completed:**\n",
    "- Loaded data from the ingested Delta table **`netflix_raw`**\n",
    "- Standardised all column names to `snake_case` using a reusable Python function  \n",
    "- Parsed and engineered key fields:\n",
    "  - Extracted `release_year` from `release_date`\n",
    "  - Converted `duration` to numeric `duration_num`\n",
    "  - Added binary flag `is_movie` for category type\n",
    "- Created a preliminary `is_hit` label (placeholder for future popularity metric integration)\n",
    "- Persisted the transformed dataset as a managed Delta table: **`netflix_clean`**\n",
    "\n",
    "**Findings:**\n",
    "- The dataset is now structured and ready for feature selection and modelling\n",
    "- Most categorical features (`rating`, `category`, `country`) contain useful variance for prediction\n",
    "- Nulls remain in descriptive fields (e.g., `director`, `cast`) but do not block training\n",
    "\n",
    "**Next steps:**\n",
    "- Proceed to **`03_modelling_and_evaluation`** to:\n",
    "  - Train a baseline classifier (e.g., Random Forest)\n",
    "  - Evaluate model performance using ROC-AUC, precision, and recall\n",
    "  - Track experiments in MLflow\n",
    "  - Generate insights and feature importance visualisations"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_feature_engineering_and_label",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
