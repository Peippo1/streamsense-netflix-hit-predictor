{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf355b28-1d36-4b10-8105-7fcbc0aac4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ¤– 03 â€“ Modelling & Evaluation\n",
    "\n",
    "**Objective:**  \n",
    "Train, evaluate, and interpret a baseline machine learning model to predict whether a Netflix title is a potential â€œhitâ€ based on its metadata features.\n",
    "\n",
    "**Scope of this notebook:**\n",
    "- Load the processed dataset from **`netflix_clean`** (created in Notebook 02)\n",
    "- Select relevant numeric and categorical features for modelling  \n",
    "- Split the dataset into training and test sets  \n",
    "- Build and train a baseline classification model (Random Forest or Logistic Regression)  \n",
    "- Evaluate model performance using key metrics:\n",
    "  - Accuracy, Precision, Recall, F1-score, ROC-AUC  \n",
    "- Log model parameters and metrics using **MLflow** for experiment tracking  \n",
    "- Analyse feature importance and interpretability  \n",
    "- Save the trained model for downstream use (e.g., predictions or dashboard integration)\n",
    "\n",
    "**Outcome:**  \n",
    "A reproducible, MLflow-tracked baseline model capable of classifying titles as *Hit* or *Non-Hit*, with performance metrics and feature insights ready for visualisation and reporting in Notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927ad802-7ce4-4490-8bb8-c1a4e79439e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the cleaned dataset created in Notebook 02\n",
    "df_spark = spark.table(\"netflix_clean\")\n",
    "\n",
    "print(f\"Row count: {df_spark.count():,}\")\n",
    "print(\"Columns:\", df_spark.columns)\n",
    "\n",
    "display(df_spark.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5b38b3-9152-4042-96a4-788b411f0a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define features and label\n",
    "\n",
    "Weâ€™ll use:\n",
    "\tâ€¢\tLabel: is_hit\n",
    "\tâ€¢\tNumeric features: release_year, duration_num, is_movie\n",
    "\tâ€¢\tCategorical features: category, rating, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7916cc2-c7e7-48ed-b286-4b09f18e67ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define feature and label columns\n",
    "label_col = \"is_hit\"\n",
    "\n",
    "numeric_features = [\"release_year\", \"duration_num\", \"is_movie\"]\n",
    "categorical_features = [\"category\", \"rating\", \"country\"]\n",
    "\n",
    "feature_cols = numeric_features + categorical_features\n",
    "\n",
    "# Drop rows with missing label or feature values\n",
    "df_spark_model = df_spark.dropna(subset=[label_col] + feature_cols)\n",
    "\n",
    "print(f\"Row count after dropping nulls: {df_spark_model.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0e96c8cd-b32d-44db-a92f-9a2de0121a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_spark_model.groupBy(label_col).count().orderBy(label_col)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b14b0a-1561-410f-b0c4-ec20dbc7d70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas for scikit-learn\n",
    "df_model = df_spark_model.select(feature_cols + [label_col]).toPandas()\n",
    "\n",
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91e8a2a-471c-4b5c-933b-cbc8ea8b4909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3944994-9976-4cfc-a7b6-d79184959db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Build pipeline: preprocessing + RandomForest\n",
    "\n",
    "We will use a ColumnTransformer for numeric/categorical handling and a RandomForestClassifier as the baseline model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6aba003-aaa2-4ee2-86e6-29e6a2d0ba31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preprocessing: scale numeric, one-hot encode categorical\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Baseline model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", rf_clf),\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3633a86f-eb9e-47e7-ae98-624e0ef74510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18724d1-37a2-4066-a593-c2780df5d77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Accuracy : {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall   : {rec:.3f}\")\n",
    "print(f\"F1-score : {f1:.3f}\")\n",
    "print(f\"ROC-AUC  : {roc_auc:.3f}\\n\")\n",
    "\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Confusion matrix plot\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7c9633-e4fe-4e8d-a441-abde0ef514fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature importance (top drivers)\n",
    "\n",
    "Weâ€™ll grab the feature names after preprocessing and show the top contributors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2799ae10-a14c-49d6-951d-eed90b22bcca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "ohe = model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"]\n",
    "encoded_cat_features = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_features = numeric_features + list(encoded_cat_features)\n",
    "\n",
    "importances = model.named_steps[\"classifier\"].feature_importances_\n",
    "\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "feat_imp.head(15)\n",
    "\n",
    "feat_imp.head(15).plot(\n",
    "    x=\"feature\",\n",
    "    y=\"importance\",\n",
    "    kind=\"barh\",\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 15 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af75b85-440a-4d7f-86ce-d6473cb8e815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“ˆ Log Model & Metrics to MLflow\n",
    "\n",
    "**Purpose:**  \n",
    "Track our experiment metadata, metrics, and model artefacts using MLflow â€” Databricksâ€™ built-in experiment tracking and model registry framework.\n",
    "\n",
    "**What this section does:**\n",
    "- Creates (or connects to) an MLflow experiment named `/Shared/StreamSense_Experiments`\n",
    "- Logs key model parameters (`n_estimators`, `max_depth`)\n",
    "- Logs performance metrics (accuracy, precision, recall, F1, ROC-AUC)\n",
    "- Captures the modelâ€™s input schema and a small example for reproducibility\n",
    "- Saves the trained scikit-learn pipeline as a versioned model artefact\n",
    "\n",
    "This ensures that each model training run is fully traceable â€” including configuration, metrics, and code version â€” enabling easy comparison, rollback, and future deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0001f6d-7404-4559-8828-5e9267c73685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Set or create an experiment; adjust path if you want it elsewhere\n",
    "mlflow.set_experiment(\"/Shared/StreamSense_Experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"baseline_random_forest\"):\n",
    "    # Log model parameters and metrics\n",
    "    mlflow.log_param(\"n_estimators\", rf_clf.n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", rf_clf.max_depth)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"precision\", prec)\n",
    "    mlflow.log_metric(\"recall\", rec)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "    # Add model signature and input example for reproducibility\n",
    "    # Cast numeric columns to float to avoid MLflow integer schema warnings\n",
    "    X_train = X_train.astype({\n",
    "        \"release_year\": \"float64\",\n",
    "        \"duration_num\": \"float64\",\n",
    "        \"is_movie\": \"float64\"\n",
    "    })\n",
    "    input_example = X_train.head(1)\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log model with metadata\n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65837ba3-6c0b-47c5-8001-67f6dd0b57d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> ðŸ’¡ **Note:**  \n",
    "> MLflow may issue a warning when logging models with integer features that could contain missing values in the future.  \n",
    "> To avoid schema mismatches, we explicitly cast numeric columns (`release_year`, `duration_num`, `is_movie`) to `float64` before inferring the model signature.  \n",
    "> This ensures robust schema enforcement during future predictions or model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d32d873-c023-432b-b59a-e3606df4caef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ§¾ Notebook Summary â€“ 03_Modelling_And_Evaluation\n",
    "\n",
    "**Objective:**  \n",
    "Train and evaluate a baseline classification model to predict whether a Netflix title is a potential \"hit\" using engineered metadata features.\n",
    "\n",
    "**Key steps completed:**\n",
    "- Loaded the cleaned dataset from the **`netflix_clean`** Delta table\n",
    "- Selected numeric (`release_year`, `duration_num`, `is_movie`) and categorical (`category`, `rating`, `country`) features\n",
    "- Split the data into training and test sets with stratified sampling\n",
    "- Built a scikit-learn pipeline with:\n",
    "  - StandardScaler for numeric features\n",
    "  - OneHotEncoder for categorical features\n",
    "  - RandomForestClassifier as the baseline model\n",
    "- Evaluated the model using accuracy, precision, recall, F1-score, and ROC-AUC\n",
    "- Inspected top feature importances to understand drivers of the `is_hit` prediction\n",
    "- Optionally logged parameters, metrics, and the model artifact in **MLflow**\n",
    "\n",
    "**Next steps:**\n",
    "- Use this model in **Notebook 04** to:\n",
    "  - Build a \"What-if\" prediction interface for hypothetical titles\n",
    "  - Create visualisations and dashboards to share insights\n",
    "  - Refine the model and features based on performance and business interpretability"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_modelling_and_evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
