{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a1c16b-785c-487e-bcc0-0f75ebafe748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# StreamSense — Hit Predictor Demo\n",
    "\n",
    "**Objective:**  \n",
    "Showcase the trained StreamSense model in action through interactive *What-If* predictions and visual insights.\n",
    "\n",
    "This notebook demonstrates how the model can estimate a title’s likelihood of becoming a “hit” based on its metadata — such as category, rating, duration, release year, and country.  \n",
    "It also provides a few visual analytics to explore how hit rates vary across the Netflix catalogue.\n",
    "\n",
    "---\n",
    "\n",
    "### Scope\n",
    "- Load the cleaned dataset (`netflix_clean`) and latest MLflow-logged model  \n",
    "- Implement a simple **What-If Prediction Helper** to score hypothetical titles  \n",
    "- Run example scenarios to test how metadata changes affect hit probability  \n",
    "- Generate visual insights on hit trends by category, rating, and release year  \n",
    "\n",
    "---\n",
    "\n",
    "### Outcome\n",
    "An interactive, demo-ready notebook that:\n",
    "- Lets users test hypothetical titles and instantly see predicted hit probabilities  \n",
    "- Displays key trends from the dataset for storytelling and presentation  \n",
    "- Serves as the final showcase notebook in the StreamSense workflow:\n",
    "  1. Data ingestion  \n",
    "  2. Feature engineering  \n",
    "  3. Model training & tracking  \n",
    "  4. **Hit predictor demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261145c0-2045-4140-a6c0-36f8e0d48841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load netflix_clean and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d962da55-63d0-44fe-a0e3-00f0497e3f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load cleaned Delta table from Notebook 02\n",
    "df_spark = spark.table(\"netflix_clean\")\n",
    "\n",
    "print(f\"Rows: {df_spark.count():,}\")\n",
    "print(\"Columns:\", df_spark.columns)\n",
    "\n",
    "display(df_spark.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e71b618-3359-49a9-8abd-8a10551847f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load latest model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0058cc3f-747c-46cb-83d4-cdee10439f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve experiment\n",
    "experiment = mlflow.get_experiment_by_name(\"/Shared/StreamSense_Experiments\")\n",
    "assert experiment is not None, \"Experiment '/Shared/StreamSense_Experiments' not found.\"\n",
    "\n",
    "# Get most recent run\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "assert len(runs_df) > 0, \"No runs found in the experiment.\"\n",
    "\n",
    "latest_run_id = runs_df.iloc[0].run_id\n",
    "model_uri = f\"runs:/{latest_run_id}/model\"\n",
    "\n",
    "print(\"Loading model from:\", model_uri)\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae2766c-1d89-43f1-a617-80d13d8a2a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### WHAT-IF PREDICTION HELPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc641bfe-2e25-4a17-ad09-44b54ab5324f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature configuration must match training\n",
    "numeric_features = [\"release_year\", \"duration_num\", \"is_movie\"]\n",
    "categorical_features = [\"category\", \"rating\", \"country\"]\n",
    "feature_cols = numeric_features + categorical_features\n",
    "\n",
    "def predict_hit_probability(\n",
    "    category: str,\n",
    "    rating: str,\n",
    "    release_year: int,\n",
    "    duration_num: int,\n",
    "    is_movie: int,\n",
    "    country: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a one-row DataFrame for a hypothetical title and return predicted hit probability.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"category\": [category],\n",
    "        \"rating\": [rating],\n",
    "        \"release_year\": [release_year],\n",
    "        \"duration_num\": [duration_num],\n",
    "        \"is_movie\": [is_movie],\n",
    "        \"country\": [country],\n",
    "    }\n",
    "    input_df = pd.DataFrame(data, columns=feature_cols)\n",
    "    \n",
    "    proba = loaded_model.predict_proba(input_df)[0, 1]\n",
    "    pred_class = loaded_model.predict(input_df)[0]\n",
    "    \n",
    "    print(\"Input:\", data)\n",
    "    print(f\"Predicted hit probability: {proba:.2%}\")\n",
    "    print(\"Predicted class:\", \"HIT (1)\" if pred_class == 1 else \"NON-HIT (0)\")\n",
    "    \n",
    "    return proba, pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c520376c-e154-4e8c-91ac-3f6f21f232e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visual Exploration\n",
    "\n",
    "Before diving into interactive predictions, this section explores overall trends in the Netflix dataset.\n",
    "\n",
    "We visualise key dimensions that correlate with a title’s hit probability:\n",
    "\n",
    "- Hit rate by **category** (Movies vs TV Shows)\n",
    "- Hit rate by **rating** (content maturity)\n",
    "- Hit rate over **release year** (temporal trends)\n",
    "- **Feature importances** from the trained model\n",
    "\n",
    "These charts help contextualise the model’s predictions and reveal the main factors influencing a title’s success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa6c4d6-e671-41a0-bf24-b921b0dc35f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregate hit rate by category\n",
    "hit_by_category = (\n",
    "    df_spark\n",
    "    .groupBy(\"category\")\n",
    "    .agg(\n",
    "        F.avg(\"is_hit\").alias(\"hit_rate\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .orderBy(F.col(\"hit_rate\").desc())\n",
    ")\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "pdf_cat = hit_by_category.toPandas()\n",
    "\n",
    "# Drop rows with null category to avoid matplotlib TypeError\n",
    "pdf_cat = pdf_cat[pdf_cat[\"category\"].notna()]\n",
    "\n",
    "# (Optional but safe) cast to string for labels\n",
    "pdf_cat[\"category\"] = pdf_cat[\"category\"].astype(str)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(pdf_cat[\"category\"], pdf_cat[\"hit_rate\"])\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Hit rate\")\n",
    "plt.title(\"Hit rate by category\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save alongside the notebook in workspace\n",
    "plt.savefig(\n",
    "    \"hit_rate_by_category.png\",\n",
    "    dpi=180,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a57973-9806-449a-bdb1-74c42a3b7c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hit rate by Release Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9de40b4-5eff-43ab-b1f7-83eef4544de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregate hit rate by release year\n",
    "hit_by_year = (\n",
    "    df_spark\n",
    "    .groupBy(\"release_year\")\n",
    "    .agg(\n",
    "        F.avg(\"is_hit\").alias(\"hit_rate\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .filter(F.col(\"release_year\").isNotNull())\n",
    "    .orderBy(\"release_year\")\n",
    ")\n",
    "\n",
    "pdf_year = hit_by_year.toPandas()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(pdf_year[\"release_year\"], pdf_year[\"hit_rate\"], marker=\"o\")\n",
    "plt.xlabel(\"Release year\")\n",
    "plt.ylabel(\"Hit rate\")\n",
    "plt.title(\"Hit rate over time (by release year)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"hit_rate_by_year.png\",\n",
    "    dpi=180,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0198140a-0c58-4c6e-8345-e1a5bfc37ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Importance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b24da7b-40c3-4610-949e-b6e6b9a0c237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# numeric_features and categorical_features already defined above\n",
    "\n",
    "preprocessor = loaded_model.named_steps[\"preprocessor\"]\n",
    "ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "encoded_cat_features = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_features = numeric_features + list(encoded_cat_features)\n",
    "\n",
    "clf = loaded_model.named_steps[\"classifier\"]\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\"feature\": all_features, \"importance\": importances})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "\n",
    "display(feat_imp.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a42e2c-0e93-4dd9-add3-968e9a81271a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature importance plot + PNG export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bf563b-8856-4126-9e88-5c38f0d30d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If not already defined earlier in the notebook, uncomment these:\n",
    "# numeric_features = [\"release_year\", \"duration_num\", \"is_movie\"]\n",
    "# categorical_features = [\"category\", \"rating\", \"country\"]\n",
    "\n",
    "# Extract feature names from the preprocessing pipeline\n",
    "preprocessor = loaded_model.named_steps[\"preprocessor\"]\n",
    "ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "encoded_cat_features = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_features = numeric_features + list(encoded_cat_features)\n",
    "\n",
    "# Extract feature importances from the classifier\n",
    "clf = loaded_model.named_steps[\"classifier\"]\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\"feature\": all_features, \"importance\": importances})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "\n",
    "# Plot top N features\n",
    "top_n = 15\n",
    "top_feat = feat_imp.head(top_n).sort_values(\"importance\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(top_feat[\"feature\"], top_feat[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(f\"Top {top_n} feature importances\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save PNG alongside your notebook (like the other plots)\n",
    "plt.savefig(\n",
    "    \"feature_importance.png\",\n",
    "    dpi=180,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11543ca-9a50-4fd3-bd37-0e094beb553f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hit rate by Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab10cd2-2c31-4d66-8c6e-b05ac816a3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregate hit rate by rating\n",
    "hit_by_rating = (\n",
    "    df_spark\n",
    "    .groupBy(\"rating\")\n",
    "    .agg(\n",
    "        F.avg(\"is_hit\").alias(\"hit_rate\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .orderBy(F.col(\"hit_rate\").desc())\n",
    ")\n",
    "\n",
    "pdf_rating = hit_by_rating.toPandas()\n",
    "\n",
    "# Drop nulls and cast to string (avoid matplotlib TypeError)\n",
    "pdf_rating = pdf_rating[pdf_rating[\"rating\"].notna()]\n",
    "pdf_rating[\"rating\"] = pdf_rating[\"rating\"].astype(str)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(pdf_rating[\"rating\"], pdf_rating[\"hit_rate\"])\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Hit rate\")\n",
    "plt.title(\"Hit rate by rating\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"hit_rate_by_rating.png\",\n",
    "    dpi=180,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a3bf78-9bcf-461b-87c4-f5428089c250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hit rate by category and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3794d43-af40-4de4-858e-03383ba0046d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hit rate by category\n",
    "hit_by_category = (\n",
    "    df_spark\n",
    "    .groupBy(\"category\")\n",
    "    .agg(F.avg(\"is_hit\").alias(\"hit_rate\"), F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(F.col(\"hit_rate\").desc())\n",
    ")\n",
    "display(hit_by_category)\n",
    "\n",
    "# Hit rate by rating\n",
    "hit_by_rating = (\n",
    "    df_spark\n",
    "    .groupBy(\"rating\")\n",
    "    .agg(F.avg(\"is_hit\").alias(\"hit_rate\"), F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(F.col(\"hit_rate\").desc())\n",
    ")\n",
    "display(hit_by_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0061e6-5f53-4666-bae3-05d4cc1e90b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hit rate over release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0fa7dd-3ac8-4e39-9fd2-6ee5d1edcf7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hit_by_year = (\n",
    "    df_spark\n",
    "    .groupBy(\"release_year\")\n",
    "    .agg(F.avg(\"is_hit\").alias(\"hit_rate\"), F.count(\"*\").alias(\"count\"))\n",
    "    .filter(F.col(\"release_year\").isNotNull())\n",
    "    .orderBy(\"release_year\")\n",
    ")\n",
    "display(hit_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c34425-8b63-4cdb-a4e6-63a6747ba55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fe1ee9-f554-4daa-93eb-6363a60c9037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Modern, popular-style movie\n",
    "predict_hit_probability(\n",
    "    category=\"Movie\",\n",
    "    rating=\"TV-MA\",\n",
    "    release_year=2023,\n",
    "    duration_num=110,\n",
    "    is_movie=1,\n",
    "    country=\"United States\"\n",
    ")\n",
    "\n",
    "# Example 2: Older children’s show\n",
    "predict_hit_probability(\n",
    "    category=\"TV Show\",\n",
    "    rating=\"TV-Y7\",\n",
    "    release_year=2012,\n",
    "    duration_num=2,\n",
    "    is_movie=0,\n",
    "    country=\"United Kingdom\"\n",
    ")\n",
    "\n",
    "# Example 3: Recent family film\n",
    "predict_hit_probability(\n",
    "    category=\"Movie\",\n",
    "    rating=\"PG\",\n",
    "    release_year=2022,\n",
    "    duration_num=95,\n",
    "    is_movie=1,\n",
    "    country=\"Canada\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16929804-2722-4099-8485-611c35d41072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Persist aggregates for dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097d7cbe-42b2-4a66-b902-83769c895587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Recompute aggregates to be safe\n",
    "hit_by_category = (\n",
    "    df_spark.groupBy(\"category\")\n",
    "    .agg(F.avg(\"is_hit\").alias(\"hit_rate\"), F.count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "hit_by_rating = (\n",
    "    df_spark.groupBy(\"rating\")\n",
    "    .agg(F.avg(\"is_hit\").alias(\"hit_rate\"), F.count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "hit_by_year = (\n",
    "    df_spark.groupBy(\"release_year\")\n",
    "    .agg(F.avg(\"is_hit\").alias(\"hit_rate\"), F.count(\"*\").alias(\"count\"))\n",
    "    .filter(F.col(\"release_year\").isNotNull())\n",
    ")\n",
    "\n",
    "# Persist as Delta tables for dashboards\n",
    "(\n",
    "    hit_by_category.write.mode(\"overwrite\").format(\"delta\")\n",
    "    .saveAsTable(\"streamsense_hit_by_category\")\n",
    ")\n",
    "(\n",
    "    hit_by_rating.write.mode(\"overwrite\").format(\"delta\")\n",
    "    .saveAsTable(\"streamsense_hit_by_rating\")\n",
    ")\n",
    "(\n",
    "    hit_by_year.write.mode(\"overwrite\").format(\"delta\")\n",
    "    .saveAsTable(\"streamsense_hit_by_year\")\n",
    ")\n",
    "\n",
    "print(\"Saved: streamsense_hit_by_category, streamsense_hit_by_rating, streamsense_hit_by_year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3141d7c7-083a-4bea-915f-a44ac05a2420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook Summary – 04 Hit Predictor Demo\n",
    "\n",
    "**Objective**  \n",
    "Demonstrate the StreamSense model through interactive predictions and visual insights.\n",
    "\n",
    "**Key steps completed**\n",
    "\n",
    "- Loaded the latest trained Random Forest model from MLflow  \n",
    "- Implemented a *What-If* predictor to simulate hit probabilities for new titles  \n",
    "- Explored hit rate patterns by **category**, **rating**, and **release year**  \n",
    "- Generated feature importance scores to understand which signals the model relies on  \n",
    "- Produced static plots (PNG) for use in the README and submission deck  \n",
    "\n",
    "**Next steps (optional)**\n",
    "\n",
    "- Wrap the what-if predictor in a lightweight UI (e.g. Streamlit or a Databricks dashboard)  \n",
    "- Replace the heuristic `is_hit` label with one derived from external sources (IMDb, TMDb)  \n",
    "- Enrich the model with text embeddings (descriptions, cast, director) for richer content understanding  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_hit_predictor_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
