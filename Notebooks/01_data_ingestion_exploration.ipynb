{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7365c1-a556-4269-a96e-aa88de661421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01 – Data Ingestion & Exploration\n",
    "\n",
    "**Project:** StreamSense – Netflix Hit Predictor  \n",
    "**Goal of this notebook:**\n",
    "- Load the raw Netflix CSV dataset from DBFS\n",
    "- Create a managed Delta table: `netflix_raw`\n",
    "- Perform initial exploratory analysis:\n",
    "  - Schema & data types\n",
    "  - Row counts, duplicates\n",
    "  - Nulls / missing data\n",
    "  - Basic distributions for key fields (type, rating, release_year, etc.)\n",
    "\n",
    "This notebook should be safe to re-run end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0e5a94-744d-4f0c-bd06-8b7b6e6a7156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "\n",
    "# Update this if your filename/path is different\n",
    "DATA_PATH = \"dbfs:/FileStore/netflix_data.csv\"  # e.g. netflix_data.csv or netflix_titles.csv\n",
    "\n",
    "print(f\"Using data from: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1028d4d1-268d-43cc-aac8-42f207ea0ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 2. Load raw CSV into Spark DataFrame\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(DATA_PATH)\n",
    ")\n",
    "\n",
    "print(f\"Row count (raw): {df_raw.count():,}\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57857662-0cc1-496f-8175-76699754207b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Save as a managed Delta table for reuse\n",
    "\n",
    "table_name = \"netflix_raw\"\n",
    "\n",
    "(\n",
    "    df_raw\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "\n",
    "print(f\"Saved table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8711f72b-eeda-44bc-8078-cbae8b56c3cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fbb3aa-f125-4eaf-b3f8-2f040680da8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"netflix_raw\")\n",
    "\n",
    "print(f\"Rows: {df.count():,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f1368a-f991-4df6-b606-d4c87c5b05b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36bea205-8f4b-4540-9796-38273dd7bc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Null counts per column\n",
    "\n",
    "null_counts = (\n",
    "    df.select([\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.col(c) == \"\"), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aa78f6-d4c6-4b74-bb91-433771559fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if \"show_id\" in df.columns:\n",
    "    dup_count = (\n",
    "        df.groupBy(\"show_id\")\n",
    "          .count()\n",
    "          .filter(F.col(\"count\") > 1)\n",
    "          .count()\n",
    "    )\n",
    "    print(f\"Duplicate show_id values: {dup_count}\")\n",
    "else:\n",
    "    print(\"No 'show_id' column found – will choose a different key later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59b2136-f25c-4ecf-a5d2-87fac1823a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col_name in [\"type\", \"rating\", \"release_year\", \"country\"]:\n",
    "    if col_name in df.columns:\n",
    "        print(f\"\\nValue counts for '{col_name}':\")\n",
    "        display(\n",
    "            df.groupBy(col_name)\n",
    "              .count()\n",
    "              .orderBy(F.col(\"count\").desc())\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nColumn '{col_name}' not found in this dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0069b2ff-1803-43c7-8b93-fe3b28d9f11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Findings\n",
    "\n",
    "- **Row count:** `<fill in from output>`\n",
    "- **Columns:** `<list key ones e.g. type, title, release_year, rating, ...>`\n",
    "- **Null patterns:**\n",
    "  - `director` has many nulls\n",
    "  - `country` partially missing\n",
    "- **Duplicates:**\n",
    "  - `show_id` seems unique (if present) / or some duplicates found\n",
    "- **Interesting distributions:**\n",
    "  - Majority of titles are Movies vs TV Shows\n",
    "  - Release years range from X to Y\n",
    "  - Ratings concentrated around TV-MA / TV-14, etc.\n",
    "\n",
    "These findings will drive:\n",
    "- How we define our `is_hit` label\n",
    "- Which features are most promising for the model"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_ingestion_exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
