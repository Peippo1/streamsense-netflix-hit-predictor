{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7365c1-a556-4269-a96e-aa88de661421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 01 â€“ Data Ingestion & Exploration\n",
    "\n",
    "**Project:** StreamSense â€“ Netflix Hit Predictor  \n",
    "**Goal of this notebook:**\n",
    "- Load the raw Netflix CSV dataset from DBFS\n",
    "- Create a managed Delta table: `netflix_raw`\n",
    "- Perform initial exploratory analysis:\n",
    "  - Schema & data types\n",
    "  - Row counts, duplicates\n",
    "  - Nulls / missing data\n",
    "  - Basic distributions for key fields (type, rating, release_year, etc.)\n",
    "\n",
    "This notebook should be safe to re-run end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0e5a94-744d-4f0c-bd06-8b7b6e6a7156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "\n",
    "# Update this if your filename/path is different\n",
    "DATA_PATH = \"dbfs:/Volumes/workspace/my_catalog/my_volume/Netflix Dataset.csv\"  # e.g. netflix_data.csv or netflix_titles.csv\n",
    "\n",
    "print(f\"Using data from: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1028d4d1-268d-43cc-aac8-42f207ea0ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 2. Load raw CSV into Spark DataFrame\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(DATA_PATH)\n",
    ")\n",
    "\n",
    "print(f\"Row count (raw): {df_raw.count():,}\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57857662-0cc1-496f-8175-76699754207b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Save as a managed Delta table for reuse\n",
    "\n",
    "table_name = \"netflix_raw\"\n",
    "\n",
    "(\n",
    "    df_raw\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "\n",
    "print(f\"Saved table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8711f72b-eeda-44bc-8078-cbae8b56c3cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM netflix_raw\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fbb3aa-f125-4eaf-b3f8-2f040680da8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"netflix_raw\")\n",
    "\n",
    "print(f\"Rows: {df.count():,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f1368a-f991-4df6-b606-d4c87c5b05b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36bea205-8f4b-4540-9796-38273dd7bc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Null counts per column\n",
    "\n",
    "null_counts = (\n",
    "    df.select([\n",
    "        F.sum(F.when(F.col(c).isNull() | (F.col(c) == \"\"), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aa78f6-d4c6-4b74-bb91-433771559fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if \"Show_Id\" in df.columns:\n",
    "    dup_count = (\n",
    "        df.groupBy(\"Show_Id\")\n",
    "          .count()\n",
    "          .filter(F.col(\"count\") > 1)\n",
    "          .count()\n",
    "    )\n",
    "    print(f\"Duplicate Show_Id values: {dup_count}\")\n",
    "else:\n",
    "    print(\"No 'Show_Id' column found â€“ will choose a different key later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d413a0c2-278c-4126-ad45-fc864edfe62b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762558339833}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\n",
    "        \"/Volumes/workspace/my_catalog/my_volume/Netflix Dataset.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59b2136-f25c-4ecf-a5d2-87fac1823a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col_name in [\"Type\", \"Rating\", \"Release_Date\", \"Country\"]:\n",
    "    if col_name in df.columns:\n",
    "        print(f\"\\nValue counts for '{col_name}':\")\n",
    "        display(\n",
    "            df.groupBy(col_name)\n",
    "              .count()\n",
    "              .orderBy(F.col(\"count\").desc())\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nColumn '{col_name}' not found in this dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0069b2ff-1803-43c7-8b93-fe3b28d9f11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Findings\n",
    "\n",
    "- **Row count:** `<fill in from output>`\n",
    "- **Columns:** `<list key ones e.g. type, title, release_year, rating, ...>`\n",
    "- **Null patterns:**\n",
    "  - `director` has many nulls\n",
    "  - `country` partially missing\n",
    "- **Duplicates:**\n",
    "  - `show_id` seems unique (if present) / or some duplicates found\n",
    "- **Interesting distributions:**\n",
    "  - Majority of titles are Movies vs TV Shows\n",
    "  - Release years range from X to Y\n",
    "  - Ratings concentrated around TV-MA / TV-14, etc.\n",
    "\n",
    "These findings will drive:\n",
    "- How we define our `is_hit` label\n",
    "- Which features are most promising for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbe66b6-dd1c-44b1-942d-ab0fa2c19042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\n",
    "        \"/Volumes/workspace/my_catalog/my_volume/Netflix Dataset.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562b1c8f-1d0c-4206-99e9-528d5bf068d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ§¾ Notebook Summary â€“ 01_Data_Ingestion_Exploration\n",
    "\n",
    "**Objective:**  \n",
    "Ingest the raw Netflix dataset into Databricks, perform an initial exploration of its structure and quality, and prepare it for downstream cleaning and feature engineering.\n",
    "\n",
    "**Key steps completed:**\n",
    "- Loaded the Kaggle Netflix dataset (`Netflix Dataset.csv`) from Databricks Volumes using PySpark  \n",
    "- Verified schema, data types, and record count  \n",
    "- Conducted basic exploratory analysis (row counts, null checks, value distributions)  \n",
    "- Identified key columns for modelling (`category`, `rating`, `release_date`, `duration`, `description`, etc.)  \n",
    "- Saved the ingested dataset as a managed Delta table: **`netflix_raw`**\n",
    "\n",
    "**Findings:**\n",
    "- Dataset contains ~[insert row count] titles with 11 columns  \n",
    "- Columns are all string-typed and require type conversion and standardisation  \n",
    "- `release_date` and `duration` fields are useful for numeric feature extraction  \n",
    "- Missing or null values present in fields like `director` and `cast`\n",
    "\n",
    "**Next steps:**\n",
    "- Proceed to **`02_feature_engineering_and_label`** to:\n",
    "  - Standardise column names to snake_case  \n",
    "  - Parse `release_year` and numeric `duration`  \n",
    "  - Add derived features (`is_movie`, `is_hit`)  \n",
    "  - Save a clean, model-ready Delta table (`netflix_clean` / `netflix_model_data`)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4846838913027925,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_ingestion_exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
